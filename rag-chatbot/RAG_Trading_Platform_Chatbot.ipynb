{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aea097",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "‚úÖ **You've built a complete RAG chatbot!**\n",
    "\n",
    "### To use locally on your machine:\n",
    "```bash\n",
    "cd rag-chatbot\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "python app.py\n",
    "# Open http://localhost:7860\n",
    "```\n",
    "\n",
    "### To share your Gradio app:\n",
    "- The URL generated above (share=True) can be shared for 72 hours\n",
    "- Deploy to Hugging Face Spaces for permanent hosting\n",
    "\n",
    "### To improve the chatbot:\n",
    "- Add more documents to the vector store\n",
    "- Adjust `chunk_size` and `k` parameters for better retrieval\n",
    "- Use `gpt-4` instead of `gpt-3.5-turbo` for better answers\n",
    "- Add custom system prompts to guide LLM behavior\n",
    "\n",
    "**Happy chatting! üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d911d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio in Colab\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57162b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio chat interface\n",
    "def chat_function(message, history):\n",
    "    \"\"\"Chat function for Gradio.\"\"\"\n",
    "    response = rag_chain.invoke({\"question\": message})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    chat_function,\n",
    "    examples=[\n",
    "        \"What are the main services in the trading platform?\",\n",
    "        \"How does order-entry service consume messages?\",\n",
    "        \"What failure scenarios are documented?\",\n",
    "        \"What are the runbook quick checks?\",\n",
    "        \"Explain Safeguard usage for SQL credentials.\"\n",
    "    ],\n",
    "    title=\"üí¨ Trading Platform Documentation Chatbot\",\n",
    "    description=\"Ask questions about the trading platform architecture, services, and operations. Answers are based on the official documentation.\",\n",
    "    theme=gr.themes.Soft(),\n",
    ")\n",
    "\n",
    "print(\"üéâ Gradio interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a414bf",
   "metadata": {},
   "source": [
    "## 7. Build Gradio Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG with sample queries\n",
    "test_queries = [\n",
    "    \"What are the main services in the trading platform?\",\n",
    "    \"How does the order-entry service work?\",\n",
    "    \"What are the common failure scenarios?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG system with sample queries...\\n\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Q{i}: {query}\")\n",
    "    response = rag_chain.invoke({\"question\": query})\n",
    "    print(f\"A{i}: {response['answer']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036306f9",
   "metadata": {},
   "source": [
    "## 6. Test RAG System with Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d63a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM\n",
    "print(f\"ü§ñ Setting up ChatOpenAI LLM (gpt-3.5-turbo)...\")\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Set up memory for conversation\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Create conversational RAG chain\n",
    "print(f\"‚õìÔ∏è Building conversational RAG chain...\")\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ RAG chain ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ca187",
   "metadata": {},
   "source": [
    "## 5. Set Up Conversational RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings with OpenAI\n",
    "print(f\"üî§ Creating embeddings with OpenAI (text-embedding-ada-002)...\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "# Create vector store with Chroma\n",
    "persist_dir = \"/tmp/trading_platform_chroma\"  # Use /tmp for Colab compatibility\n",
    "print(f\"üíæ Building Chroma vector store at {persist_dir}...\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=\"trading_platform\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store ready with {len(chunks)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8b56a",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings and Build Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd49d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents for better retrieval\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "print(f\"üìä Chunking {len(all_documents)} documents (size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(all_documents)\n",
    "print(f\"‚úÖ Created {len(chunks)} total chunks\")\n",
    "print(f\"üìã Sample chunk from first document:\\n{chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5efe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from all service documentation URLs\n",
    "docs_urls = [\n",
    "    \"https://raw.githubusercontent.com/somakalla1-droid/RAG/main/docs/trading-platform-doc.md\",\n",
    "    \"https://raw.githubusercontent.com/somakalla1-droid/RAG/main/docs/order-validate-doc.md\",\n",
    "    \"https://raw.githubusercontent.com/somakalla1-droid/RAG/main/docs/order-entry-doc.md\",\n",
    "    \"https://raw.githubusercontent.com/somakalla1-droid/RAG/main/docs/order-router-doc.md\",\n",
    "    \"https://raw.githubusercontent.com/somakalla1-droid/RAG/main/docs/fix-service-doc.md\",\n",
    "    \"https://raw.githubusercontent.com/somakalla1-droid/RAG/main/docs/service-registry-doc.md\",\n",
    "]\n",
    "\n",
    "all_documents = []\n",
    "for url in docs_urls:\n",
    "    print(f\"üì• Loading {url.split('/')[-1]}...\")\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"   ‚úÖ Loaded {len(documents)} document(s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error loading: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Total documents loaded: {len(all_documents)}\")\n",
    "print(f\"üìä Total content length: {sum(len(doc.page_content) for doc in all_documents):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23450249",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"üîë Enter your OpenAI API key:\")\n",
    "    api_key = getpass(\"OpenAI API Key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "print(f\"‚úÖ OpenAI API key configured (starts with: {api_key[:6]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ded1a",
   "metadata": {},
   "source": [
    "## 2. Configure OpenAI API Client\n",
    "\n",
    "Add your OpenAI API key below. Get one from https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ba2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import Document\n",
    "import gradio as gr\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac712a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain==0.1.0\",\n",
    "    \"langchain-community==0.0.10\",\n",
    "    \"langchain-openai==0.0.5\",\n",
    "    \"chromadb==0.3.21\",\n",
    "    \"sentence-transformers==2.2.2\",\n",
    "    \"requests==2.31.0\",\n",
    "    \"gradio==4.0.0\",\n",
    "    \"python-dotenv==1.0.0\"\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", package])\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06841c4b",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa003eb",
   "metadata": {},
   "source": [
    "# RAG Trading Platform Documentation Chatbot\n",
    "\n",
    "**Build a Retrieval-Augmented Generation (RAG) chatbot** that answers questions about the trading platform using:\n",
    "- **LangChain** for orchestration\n",
    "- **OpenAI API** for embeddings and LLM\n",
    "- **Chroma DB** for vector storage\n",
    "- **Gradio** for UI\n",
    "\n",
    "This notebook is optimized for **Google Colab** with easy local setup."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
