# Quick Start Guide - Single-URL RAG Assignment

## ğŸš€ Get Started in 3 Minutes

### Option 1: Google Colab (No Setup Required) â­ RECOMMENDED

1. **Open the notebook**:
   - Navigate to: `RAG_Single_URL_Assignment.ipynb`
   - Click "Open in Colab" (or upload to Colab)

2. **Run the cells**:
   ```
   Cell 1: Install dependencies (!pip install ...)
   Cell 2: Import libraries
   Cell 3: Enter your OpenAI API key
   Cell 4-7: Run through requirements 1-4
   Cell 8-9: Test with sample queries
   Cell 10: Launch Gradio UI
   ```

3. **Ask questions**:
   - Use the Gradio chat interface
   - Share the public link with others

---

### Option 2: Local Python CLI

**Prerequisites**:
```bash
python3 --version  # Need Python 3.9+
export OPENAI_API_KEY="sk-..."
```

**Installation**:
```bash
cd /Users/somakalla/Desktop/IK/Assignment-1/RAG/rag-chatbot
pip install -r requirements.txt
```

**Run the chatbot**:
```bash
python -m src.rag_pipeline_single_url
```

**Example interaction**:
```
> You: What is this document about?
> Assistant: The document discusses...

> You: Tell me more about security
> Assistant: Regarding the security aspects mentioned...

> You: exit
```

---

### Option 3: Local Web UI with Gradio

**Run the web app**:
```bash
cd /Users/somakalla/Desktop/IK/Assignment-1/RAG/rag-chatbot
export OPENAI_API_KEY="sk-..."
python app_single_url.py
```

**Access the UI**:
- Open browser to `http://localhost:7860`
- Chat with the bot
- No public link in local mode

---

## ğŸ“‹ What You Get

### âœ… Requirement 1: Document Chunking
```
Input: Single webpage URL
â†“
RecursiveCharacterTextSplitter (1000 chars, 200 overlap)
â†“
Output: 6-120+ semantic chunks
```

### âœ… Requirement 2: Embeddings
```
Input: Text chunks
â†“
OpenAI (text-embedding-ada-002)
â†“
Output: Semantic vectors for each chunk
```

### âœ… Requirement 3: Vector Store
```
Input: Embedding vectors
â†“
Chroma DB (local, persistent)
â†“
Output: Indexed, searchable vector database
```

### âœ… Requirement 4: LLM Chain
```
User Question
   â†“
Query embedding
   â†“
Semantic search (top 3 chunks)
   â†“
ChatOpenAI (gpt-3.5-turbo)
   â†“
Answer with context
```

### âœ… Extra: Conversational Memory
```
Your first question â†’ System remembers context
Your follow-up question â†’ System uses previous context
â†’ Proper multi-turn conversation!
```

---

## ğŸ”‘ Getting Your OpenAI API Key

1. Visit: https://platform.openai.com/api-keys
2. Sign up or log in
3. Click "Create new secret key"
4. Copy the key
5. In Colab: Enter when prompted
6. In CLI: `export OPENAI_API_KEY="sk-..."`

---

## ğŸ“ Sample Queries to Try

```
"What is the main topic?"
"Can you summarize this?"
"What are the key concepts?"
"Tell me more about [specific topic]"
"How does [feature] work?"
```

---

## ğŸ¯ Change the URL

To use a different webpage:

**Colab**:
```python
# In Cell 4 (Step 4), change:
url = "https://your-website.com/page"
```

**CLI**:
```python
# Edit src/rag_pipeline_single_url.py, change line ~270:
url = "https://your-website.com/page"
```

---

## ğŸ› Troubleshooting

### "OpenAI API key not found"
â†’ Make sure you set the environment variable or entered it in Colab

### "Module not found: chromadb"
â†’ Run: `pip install chromadb`

### "Timeout loading URL"
â†’ Make sure the URL is accessible (try in browser first)

### "Connection refused" (Gradio)
â†’ Another app is using port 7860. Restart or kill the process

---

## ğŸ“Š What's Happening Behind the Scenes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Question: "What is X?"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. Generate embedding     â”‚
    â”‚     of your question       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. Search vector store    â”‚
    â”‚     for similar content    â”‚
    â”‚     (top 3 chunks)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. Create context prompt: â”‚
    â”‚     "Here's what I found:" â”‚
    â”‚     [Chunk 1]              â”‚
    â”‚     [Chunk 2]              â”‚
    â”‚     [Chunk 3]              â”‚
    â”‚     "Now answer: What is X?"
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  4. Send to ChatOpenAI     â”‚
    â”‚     (gpt-3.5-turbo)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Your Answer: "[Context-grounded      â”‚
â”‚     response based on document]"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Expected Performance

| Metric | Expected |
|--------|----------|
| Loading webpage | 2-5 seconds |
| Chunking document | <1 second |
| Generating embeddings | 5-10 seconds |
| Setting up LLM chain | 2-3 seconds |
| Answering a question | 2-5 seconds |

**Total initialization**: ~15-25 seconds (first time only)  
**Per question**: ~3 seconds

---

## ğŸ’¡ Pro Tips

1. **Longer documents**: May take longer to chunk and embed, but answer quality improves
2. **Better questions**: More specific questions â†’ better answers
3. **Cost optimization**: Embeddings are cheaper than multiple LLM calls
4. **Reusability**: Vector store persists, no need to re-embed on restart

---

## ğŸ“š Full Documentation

- **Detailed requirements**: See `ASSIGNMENT_SUBMISSION.md`
- **Test results**: See `TEST_REPORT.md`
- **API reference**: See docstrings in `src/rag_pipeline_single_url.py`

---

## âœ… Ready to Submit?

Check this list:
- [ ] Opened Colab notebook
- [ ] Set OpenAI API key
- [ ] Ran through all cells
- [ ] Tested with sample queries
- [ ] Launched Gradio UI
- [ ] All working! ğŸ‰

---

**Questions?** Check the troubleshooting section above or refer to detailed documentation.

**Happy coding!** ğŸš€
